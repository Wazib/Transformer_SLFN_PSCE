{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_SLFN_PSCE",
      "provenance": [],
      "collapsed_sections": [
        "Z-SVfu0kalTi",
        "9gXV-3AMS3KJ",
        "JtzNGGNQ25oA",
        "L9A-PsxQ7HPs",
        "PEzxlUmi9FqD",
        "zoEPMLyv_MKD",
        "FNi-YzS3S3KO",
        "LjggRfX3S3KR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wazib/Transformer_SLFN_PSCE/blob/main/Transformer_SLFN_PSCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnMEFP2yS3KI"
      },
      "source": [
        "# Text Classification using Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-SVfu0kalTi"
      },
      "source": [
        "#Downloading spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wioX-kCL83kl"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gXV-3AMS3KJ"
      },
      "source": [
        "# Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_zJvhrS3KL"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K \n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AKgL-4BBWMe"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixgszfcp_Xiz"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize, WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ko-C0e8eN0"
      },
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rus_qsnLX94t"
      },
      "source": [
        "#Loading Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Kq0tdnSvl2"
      },
      "source": [
        "# IMDB Movie Reviews\n",
        "df = pd.read_csv('IMDB Dataset.csv', engine='python',index_col=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmKpoCJbwYVr"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4zqC6j_dVQw"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtzNGGNQ25oA"
      },
      "source": [
        "#Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waWsZuX2MYGR"
      },
      "source": [
        "Adding start, end and sep token as sostoken, eostoken, septoken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6xYCAJ9M8ED"
      },
      "source": [
        "def special_token(ipstr):\n",
        "    #Adding the start token and end token\n",
        "    ipstr='sostoken '+ipstr+' eostoken'\n",
        "    #Adding septoken for new sentences\n",
        "    ipstr=re.sub('[.]+', ' septoken ', ipstr)\n",
        "    #Checking for the case where septoken is also the eostoken\n",
        "    ipstr=re.sub('septoken  eostoken', 'eostoken', ipstr)\n",
        "    return ipstr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtzP5hQSI7A"
      },
      "source": [
        "#Adding the special tokens\n",
        "text_special=[special_token(rev) for rev in df.review]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4MbU296jhjt"
      },
      "source": [
        "> Cleaning, lemmatizing and filtering length\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzrvLEnpjNoc"
      },
      "source": [
        "#Function for text cleaning, removing URL and \n",
        "#producing lowercase words with minimal punctuation\n",
        "def cleaning(ipstr):      \n",
        "    exclude = set(string.punctuation)\n",
        "\n",
        "    # remove new line and digits with regular expression\n",
        "    ipstr = re.sub(r'\\n', '', ipstr)\n",
        "    #ipstr = re.sub(r'\\d', '', ipstr)\n",
        "    # remove patterns matching url format\n",
        "    url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?'\n",
        "    ipstr = re.sub(url_pattern, ' ', ipstr)\n",
        "    # remove non-ascii characters\n",
        "    ipstr = ''.join(character for character in ipstr if ord(character) < 128)\n",
        "    # remove punctuations\n",
        "    ipstr = ''.join(character for character in ipstr if character not in exclude)\n",
        "    # standardize white space\n",
        "    ipstr = re.sub(r'\\s+', ' ', ipstr)\n",
        "\n",
        "    # drop capitalization\n",
        "    ipstr = ipstr.lower()\n",
        "    #remove white space\n",
        "    ipstr = ipstr.strip()\n",
        "    \n",
        "    return ipstr\n",
        "\n",
        "def lenfilter(token_list):\n",
        "#    Remove single and double characters\n",
        "    return [word for word in token_list if len(word)>0]\n",
        "\n",
        "#Lemmatizing words\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "def token_lemmatize(token_list):\n",
        "    return [lemmatizer.lemmatize(t) for t in token_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKcV8sD7FXhD"
      },
      "source": [
        "#Cleaning Text\n",
        "text_clean=[cleaning(sen) for sen in text_special]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4lRitNI5emT"
      },
      "source": [
        "#Tokenizing\n",
        "tokens = [word_tokenize(sen) for sen in text_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GFoCgwz_LDF"
      },
      "source": [
        "#Filtering single and double characters\n",
        "len_filtered_words = [lenfilter(sen) for sen in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLIGpgiR9dlg"
      },
      "source": [
        "result = [' '.join(sen) for sen in len_filtered_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "788SrrxJ9dlg"
      },
      "source": [
        "df['Text_Final'] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fBil8lX9dlh"
      },
      "source": [
        "df['Tokens'] = len_filtered_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMLF-avV6KS8"
      },
      "source": [
        "Converting output to categorical format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UKzgjg96ZN1"
      },
      "source": [
        "pos_list=[]\n",
        "neg_list=[]\n",
        "for i in df['sentiment']:\n",
        "    if i=='positive':\n",
        "        pos_list.append(1)\n",
        "        neg_list.append(0)\n",
        "    elif i=='negative':\n",
        "        pos_list.append(0)\n",
        "        neg_list.append(1)\n",
        "df['pos']=pos_list\n",
        "df['neg']=neg_list\n",
        "label_names=['pos','neg']\n",
        "y_cat=df[label_names].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5wKPMvSnww"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9A-PsxQ7HPs"
      },
      "source": [
        "#Statistically Analyzing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8j-z7BxlxjN"
      },
      "source": [
        "Finding the distribution of length of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsd6TnTO7LpJ"
      },
      "source": [
        "text_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in df['Text_Final']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.xlabel('Sequence Length')  \n",
        "plt.ylabel('Occurrences')  \n",
        "plt.title('Distribution of Sequence Length')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6tybSN07o9l"
      },
      "source": [
        "Percentage of text having length below a specified value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fXIsbAv7aEY"
      },
      "source": [
        "cnt=0\n",
        "for i in df['Text_Final']:\n",
        "    if(len(i.split())<=512):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(df['Text_Final']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrT6K97lfCsq"
      },
      "source": [
        "Finding the total number of words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nlde5wLfCEo"
      },
      "source": [
        "wcnt=0\n",
        "for i in df['Text_Final']:\n",
        "    wcnt+=len(i.split())\n",
        "print(\"The total number of words are:\", wcnt, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwA0HWs7z8o"
      },
      "source": [
        "Fixing the maximum length of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSQhF7kW72VA"
      },
      "source": [
        "maxlen=512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iffee2zFZQ0L"
      },
      "source": [
        "> Dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3G6ANhBJK9X"
      },
      "source": [
        "# Create the term dictionary of our corpus, where every unique term is assigned an index\n",
        "dictionary = corpora.Dictionary(len_filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IWTr7Mvl7i7"
      },
      "source": [
        "Finding the size of vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx5zLN47myWX"
      },
      "source": [
        "#total vocabulary size before filtering\n",
        "print(\"Total vocabulary size: \", len(dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCMUADIJmL1L"
      },
      "source": [
        "vocab_size=len(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEzxlUmi9FqD"
      },
      "source": [
        "#Preparing the Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVrDZN5F8vkk"
      },
      "source": [
        "#Splitting into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_test,y_tr,y_test=train_test_split(np.array(df['Text_Final']),y_cat,test_size=0.10,random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E_AB742SzLH"
      },
      "source": [
        "#Further splitting the train set into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(x_tr,y_tr,test_size=0.10,random_state=42,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoEPMLyv_MKD"
      },
      "source": [
        "#Tokenizing the Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc3azyom2FT3"
      },
      "source": [
        "#prepare a tokenizer for reviews\n",
        "x_tokenizer = Tokenizer(num_words=vocab_size, oov_token=None) \n",
        "x_tokenizer.fit_on_texts(list(x_tr)+list(x_val)+list(x_test))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "x_test_seq   =   x_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "word_index=x_tokenizer.word_index\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=maxlen, padding='post', truncating='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=maxlen, padding='post', truncating='post')\n",
        "x_test  =   pad_sequences(x_test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "#vocab_size   =  len(x_tokenizer.word_index) + 1\n",
        "vocab_size+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke3A8lRE-3TC"
      },
      "source": [
        "#Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM-RDsjS3KN"
      },
      "source": [
        "## Implement multi head self attention as a Keras layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysz4Lzg-S3KO"
      },
      "source": [
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNi-YzS3S3KO"
      },
      "source": [
        "## Implement a Transformer block as a layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFq6hoF_eAAH"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        #Selective Learn and Forget Layer\n",
        "        self.sl1=keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"sigmoid\"), layers.Dense(embed_dim),])\n",
        "        \n",
        "        #Feed forward layer customized for Selective Learning\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"tanh\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        out2=self.sl1(out1)\n",
        "        ffn_output = out2*self.ffn(out1)\n",
        "\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhJ6_FM6S3KO"
      },
      "source": [
        "## Implement embedding layer\n",
        "\n",
        "Three tier embedding i. Token embedding, ii. Position Embedding iii. Part of Speech embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-LOT6AwZwlE"
      },
      "source": [
        "#Training Word2Vec model upon the vocabulary\n",
        "word2vec_model = gensim.models.Word2Vec(len_filtered_words, size=32, min_count = 1, window = 5)\n",
        "word_model=word2vec_model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRU-mez2YmyL"
      },
      "source": [
        "Part of Speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxLTuxhXShuP"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjf21JTnPFih"
      },
      "source": [
        "part_of_speech=[]\n",
        "for x in tqdm(range(len(df['Text_Final']))):\n",
        "    part_of_speech_t=[]\n",
        "    if len(str(df['Text_Final'][x])) != 0:\n",
        "        lines = str(df['Text_Final'][x]).split('septoken ')\n",
        "        for line in lines:\n",
        "            if line.find('eostoken', 0, len(line))== (-1):\n",
        "                line+='septoken'\n",
        "            doc = nlp(line)\n",
        "            for t in doc:\n",
        "                if (t.text=='sostoken') or (t.text=='eostoken') or (t.text=='septoken'):\n",
        "                    part_of_speech_t.append('SPCL') #Adding POS for special token\n",
        "                else:\n",
        "                    part_of_speech_t.append(t.pos_)\n",
        "    else:\n",
        "        print(\"text length = 0\")\n",
        "    part_of_speech.append(part_of_speech_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaz5zZVAjCKf"
      },
      "source": [
        "Finding unique parts of speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXiG9UBlkIO6"
      },
      "source": [
        "unique_pos=[]\n",
        "pos_dictionary = corpora.Dictionary(part_of_speech)\n",
        "for k in pos_dictionary:\n",
        "    unique_pos.append(pos_dictionary[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEnzlg6NqS9j"
      },
      "source": [
        "unique_pos=['ADJ', 'ADV', 'AUX', 'DET', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SPCL', 'VERB', 'INTJ', 'SCONJ', 'CCONJ', 'ADP', 'X', 'PUNCT', 'SYM']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6-Pt088kk3y"
      },
      "source": [
        "print(unique_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6uFEmhr_G0"
      },
      "source": [
        "\n",
        "One hot encoding the POS tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrR7WS9Bqoeq"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(unique_pos)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FkYttOQRtm_"
      },
      "source": [
        "Mapping one-hot encoding to POS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FvDlz9ssL8L"
      },
      "source": [
        "#Mapping one-hot encoding to POS\n",
        "pos_dict=dict(zip(unique_pos, onehot_encoded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Ij6UgrvNIv"
      },
      "source": [
        "word_base=word_model.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szwKF28WTQpw"
      },
      "source": [
        "Determining POS for terms in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BxZ7hovTJ07"
      },
      "source": [
        "word_pos=dict()\n",
        "word_posa=dict()\n",
        "for i in range(len(len_filtered_words)):\n",
        "    for j in range(len(len_filtered_words[i])):\n",
        "        if word_base:\n",
        "            if len_filtered_words[i][j] in word_base:\n",
        "                # Mapping part of speech vector\n",
        "                word_pos[len_filtered_words[i][j]]=pos_dict[part_of_speech[i][j]]\n",
        "                # Mapping part of speech\n",
        "                word_posa[len_filtered_words[i][j]]=part_of_speech[i][j]\n",
        "                word_base.pop(len_filtered_words[i][j])\n",
        "        else:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZyQ26CNaUMG"
      },
      "source": [
        "Part of Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJX8-t7BmzpJ"
      },
      "source": [
        "word2vec_model = gensim.models.Word2Vec(len_filtered_words, size=32, min_count = 1, window = 5)\n",
        "word_model=word2vec_model.wv\n",
        "word_base=word_model.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-schtA3rYRQ"
      },
      "source": [
        "POS Embedding Vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ZxKuC-1o-S"
      },
      "source": [
        "#Preparing a list of word vectors corresponding to each part of speech\n",
        "\n",
        "adj_list=[]\n",
        "adv_list=[]\n",
        "aux_list=[]\n",
        "det_list=[]\n",
        "noun_list=[]\n",
        "num_list=[]\n",
        "part_list=[]\n",
        "pron_list=[]\n",
        "propn_list=[]\n",
        "spcl_list=[]\n",
        "verb_list=[]\n",
        "intj_list=[]\n",
        "sconj_list=[]\n",
        "cconj_list=[]\n",
        "adp_list=[]\n",
        "x_list=[]\n",
        "punct_list=[]\n",
        "sym_list=[]\n",
        "\n",
        "for i in word_base:\n",
        "    if (word_posa[i]=='ADJ'):\n",
        "        adj_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='ADV'):\n",
        "        adv_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='AUX'):\n",
        "        aux_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='DET'):\n",
        "        det_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='NOUN'):\n",
        "        noun_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='NUM'):\n",
        "        num_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='PART'):\n",
        "        part_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='PRON'):\n",
        "        pron_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='PROPN'):\n",
        "        propn_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='SPCL'):\n",
        "        spcl_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='VERB'):\n",
        "        verb_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='INTJ'):\n",
        "        intj_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='SCONJ'):\n",
        "        sconj_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='CCONJ'):\n",
        "        cconj_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='ADP'):\n",
        "        adp_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='X'):\n",
        "        x_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='PUNCT'):\n",
        "        punct_list.append(word_model[i])\n",
        "    elif (word_posa[i]=='SYM'):\n",
        "        sym_list.append(word_model[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjzySmWoK0Gb"
      },
      "source": [
        "#This function takes the input as a list of vectors having the same POS\n",
        "# and calculates the output as the dimension-wise average of the vectors\n",
        "def avg_vector(pos_type):\n",
        "    avg_vec=[]\n",
        "    try:\n",
        "        for i in range(len(pos_type[0])):\n",
        "            avg_vec_t=[]\n",
        "            for j in range(len(pos_type)):\n",
        "                avg_vec_t.append(pos_type[j][i])\n",
        "            avg_vec_t=np.mean(avg_vec_t)\n",
        "            avg_vec.append(avg_vec_t)\n",
        "        return avg_vec\n",
        "    except:\n",
        "        avg_vec=np.zeros(32, dtype=int)\n",
        "        return list(avg_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta4pS4M3FzHf"
      },
      "source": [
        "#Determining the POS vector\n",
        "adj_pos_vector=avg_vector(adj_list)\n",
        "adv_pos_vector=avg_vector(adv_list)\n",
        "aux_pos_vector=avg_vector(aux_list)\n",
        "det_pos_vector=avg_vector(det_list)\n",
        "noun_pos_vector=avg_vector(noun_list)\n",
        "num_pos_vector=avg_vector(num_list)\n",
        "part_pos_vector=avg_vector(part_list)\n",
        "pron_pos_vector=avg_vector(pron_list)\n",
        "propn_pos_vector=avg_vector(propn_list)\n",
        "spcl_pos_vector=avg_vector(spcl_list)\n",
        "verb_pos_vector=avg_vector(verb_list)\n",
        "intj_pos_vector=avg_vector(intj_list)\n",
        "sconj_pos_vector=avg_vector(sconj_list)\n",
        "cconj_pos_vector=avg_vector(cconj_list)\n",
        "adp_pos_vector=avg_vector(adp_list)\n",
        "x_pos_vector=avg_vector(x_list)\n",
        "punct_pos_vector=avg_vector(punct_list)\n",
        "sym_pos_vector=avg_vector(sym_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9HIw9DIILJ"
      },
      "source": [
        "#Combining POS vector as a list\n",
        "pos_vector_list=[]\n",
        "pos_vector_list.append(adj_pos_vector)\n",
        "pos_vector_list.append(adv_pos_vector)\n",
        "pos_vector_list.append(aux_pos_vector)\n",
        "pos_vector_list.append(det_pos_vector)\n",
        "pos_vector_list.append(noun_pos_vector)\n",
        "pos_vector_list.append(num_pos_vector)\n",
        "pos_vector_list.append(part_pos_vector)\n",
        "pos_vector_list.append(pron_pos_vector)\n",
        "pos_vector_list.append(propn_pos_vector)\n",
        "pos_vector_list.append(spcl_pos_vector)\n",
        "pos_vector_list.append(verb_pos_vector)\n",
        "pos_vector_list.append(intj_pos_vector)\n",
        "pos_vector_list.append(sconj_pos_vector)\n",
        "pos_vector_list.append(cconj_pos_vector)\n",
        "pos_vector_list.append(adp_pos_vector)\n",
        "pos_vector_list.append(x_pos_vector)\n",
        "pos_vector_list.append(punct_pos_vector)\n",
        "pos_vector_list.append(sym_pos_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNWUsjPMWvZ"
      },
      "source": [
        "#Mapping POS vector encoding to POS\n",
        "pos_dict_v=dict(zip(unique_pos, pos_vector_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEiieVraTsa"
      },
      "source": [
        "word_model1=dict()\n",
        "for word in word_base:\n",
        "    word_model1[word]=np.concatenate((word_model[word],pos_dict_v[word_posa[word]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkFC6vznsb6m"
      },
      "source": [
        "Preparing the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPx3QBxRsu8q"
      },
      "source": [
        "embed_dim=64 #For vector POS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FnhzphYsbJ7"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_base) + 1, embed_dim))\n",
        "for word in word_base:\n",
        "    embedding_vector = word_model1[word]\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[word_index[word]] = embedding_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzLzTXjEM9oc"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights=[embedding_matrix], input_length=maxlen)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjggRfX3S3KR"
      },
      "source": [
        "## Classifier model using transformer layer\n",
        "\n",
        "Transformer layer outputs one vector for each time step of our input sequence.\n",
        "Here, we take the mean across all time steps and\n",
        "use a feed forward network on top of it to classify text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6KYPy4u4cQ7"
      },
      "source": [
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "#embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) #Method 2\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = transformer_block(x)\n",
        "x = transformer_block(x)\n",
        "x = transformer_block(x)\n",
        "x = transformer_block(x)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81aovTlOYits"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVjtabEaS3KR"
      },
      "source": [
        "# Train and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnUhO-H5Cbbw"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp = ModelCheckpoint(filepath='trfr_best_model.h5', monitor='val_categorical_accuracy', save_weights_only=True, verbose=1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "tXE8RSsBmVec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybiq1DyNAvLP"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt=tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ],
      "metadata": {
        "id": "nn-GM2bfz_yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-qkcT7xS3KR"
      },
      "source": [
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
        "history = model.fit(\n",
        "    x_tr, y_tr, batch_size=32, epochs=10, callbacks=[es,cp], validation_data=(x_val, y_val)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgzaHudRnbyN"
      },
      "source": [
        "# Evaluation of Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL8cGQabCnNQ"
      },
      "source": [
        "y_hat=model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-3-6lrz6yh"
      },
      "source": [
        "Converting y_val to 1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH1eDluMKG19"
      },
      "source": [
        "y_sent=[]\n",
        "for i in y_test:\n",
        "    y_sent.append(1-np.argmax(i)) #This is done to represent argmax=0 as positve and argmax=1 as negative."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1btueNGFTo"
      },
      "source": [
        "y_hat_sent=[]\n",
        "for i in y_hat:\n",
        "    y_hat_sent.append(1-np.argmax(i)) #This is done to represent argmax=0 as positve and argmax=1 as negative."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAbE9scn646Z"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from itertools import chain\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, threshold = roc_curve(y_sent, y_hat[:,0])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#This part is for the confusion matrix and accuracy score\n",
        "cf_matrix = confusion_matrix(y_sent, y_hat_sent) \n",
        "print ('Confusion Matrix :')\n",
        "print(cf_matrix) \n",
        "print ('Accuracy Score :',accuracy_score(y_sent, y_hat_sent))\n",
        "print ('Report : ')\n",
        "print (classification_report(y_sent, y_hat_sent) )\n",
        "#\n",
        "\n",
        "plt.figure(figsize=(8,7))\n",
        "plt.plot(fpr, tpr, label='Model (area = %0.3f)' % roc_auc, linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=18)\n",
        "plt.ylabel('True Positive Rate', fontsize=18)\n",
        "plt.title('Receiver operating characteristic: is positive', fontsize=18)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}